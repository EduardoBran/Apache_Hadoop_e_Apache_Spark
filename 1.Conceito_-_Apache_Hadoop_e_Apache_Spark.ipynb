{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f4bafe",
   "metadata": {},
   "source": [
    "# <center><span style=\"font-size: 42px;color: darkgreen;\">Conceito Apache Hadoop e Apache Spark </center></span>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Hadoop x Spark\n",
    "\n",
    "<br>\n",
    "\n",
    "O **Apache Hadoop** é uma plataforma de software open source que permite o processamento distribuído de grandes conjuntos de dados em clusters de computadores. Ele é composto por vários componentes principais, incluindo o **HDFS** (Hadoop Distributed File System), que armazena dados de forma distribuída, e o **MapReduce**, que é o modelo de programação para processamento paralelo dos dados. O **Hadoop** é amplamente utilizado para armazenar e processar dados em larga escala, especialmente em ambientes de Big Data, e é conhecido por sua capacidade de lidar com dados estruturados e não estruturados.\n",
    "\n",
    "O **Apache Spark** é uma plataforma de processamento de dados distribuída e em memória, projetada para ser mais rápida e eficiente que o Hadoop, especialmente para tarefas complexas de análise de dados. Ele suporta diversos modelos de processamento, como **processamento em batch**, **streaming** (fluxos de dados em tempo real), **machine learning** e **SQL**. Ao contrário do Hadoop MapReduce, que grava resultados intermediários no disco, o Spark processa os dados na memória, o que o torna muito mais rápido para determinadas tarefas. Spark é frequentemente utilizado para análises mais rápidas e interativas em ambientes de Big Data.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Diferenças e Similaridades entre Hadoop e Spark\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Diferenças\n",
    "\n",
    "- **Processamento**: O **Hadoop** usa o modelo **MapReduce**, que processa os dados em etapas e grava os resultados intermediários no disco. Em contrapartida, o **Spark** processa dados em **memória**, permitindo um desempenho muito mais rápido, especialmente para tarefas repetitivas ou interativas.\n",
    "  \n",
    "- **Velocidade**: Devido ao processamento em memória, o Spark é significativamente mais rápido que o Hadoop MapReduce, particularmente em tarefas complexas de análise de dados, como machine learning e streaming.\n",
    "\n",
    "- **Facilidade de uso**: O Spark possui uma API mais amigável e suporta várias linguagens de programação, como **Python**, **Scala**, **Java** e **R**, enquanto o Hadoop MapReduce tradicional é mais limitado e geralmente escrito em Java.\n",
    "\n",
    "- **Armazenamento de Dados**: Hadoop utiliza o **HDFS** (Hadoop Distributed File System) para armazenar dados de forma distribuída. Spark, por outro lado, não possui um sistema de arquivos próprio e é frequentemente usado com o HDFS ou outros sistemas de armazenamento distribuído, como o **Amazon S3**.\n",
    "\n",
    "- **Tolerância a falhas**: Ambos possuem mecanismos de tolerância a falhas, mas o Spark usa um modelo de recuperação através do **RDDs** (Resilient Distributed Datasets), que permite reconstruir dados em caso de falha. O Hadoop, por sua vez, reprocessa a etapa falhada do MapReduce a partir do disco.\n",
    "\n",
    "#### Similaridades\n",
    "\n",
    "- **Processamento distribuído**: Ambos são projetados para processar grandes volumes de dados distribuídos em clusters de computadores, aproveitando a escalabilidade horizontal.\n",
    "  \n",
    "- **Componentes do ecossistema Big Data**: Hadoop e Spark são componentes centrais em ambientes de Big Data, e ambos podem ser integrados com outras ferramentas, como **Hive**, **HBase** e **Pig**, para enriquecer o ambiente de análise de dados.\n",
    "\n",
    "- **Open Source**: Tanto Hadoop quanto Spark são plataformas open source, mantidas pela **Apache Software Foundation**.\n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Hadoop e Spark Juntos\n",
    "\n",
    "<br>\n",
    "\n",
    "O **Hadoop** e o **Spark** são frequentemente utilizados em conjunto para aproveitar o melhor dos dois mundos. O `Hadoop` fornece uma infraestrutura de armazenamento escalável e econômica com o **HDFS**, enquanto o `Spark` oferece um mecanismo de processamento rápido e versátil. Juntos, eles permitem que as organizações armazenem e processem dados em grandes volumes de forma eficiente.\n",
    "\n",
    "Por exemplo, o `Hadoop` pode ser usado para armazenar dados históricos em um cluster **HDFS**, enquanto o `Spark` é utilizado para processar e analisar esses dados em tempo real ou para executar algoritmos de machine learning. Essa combinação é ideal para cenários onde o `Hadoop` gerencia o armazenamento de dados a longo prazo e o `Spark` realiza o processamento rápido.\n",
    "\n",
    "É possível usar um sem o outro!\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Como o Spark Funciona Sobre o HDFS\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Para <u>`ler`</u> os arquivos do HDFS com Spark usamos:\n",
    "\n",
    "```bash\n",
    "textfile = sc.textFile(\"hdfs://mycluster/data/file.txt\")\n",
    "```\n",
    "\n",
    "#### Para <u>`gravar`</u> os arquivos do HDFS com Spark usamos:\n",
    "\n",
    "```bash\n",
    "myRDD.saveAsTextFile(\"hdfs://mycluster/data/output.txt\")\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Modos de Instalação do Spark\n",
    "\n",
    "<br>\n",
    "\n",
    "O **Spark** pode ser instalado e executado em três modos distintos, dependendo da infraestrutura e das necessidades de processamento:\n",
    "\n",
    "- **Standalone**: Neste modo, o Spark opera de forma independente, sem a necessidade de um cluster gerenciador externo. Ele é ideal para pequenos clusters ou para desenvolvimento e testes locais, sendo mais fácil de configurar e gerenciar.\n",
    "\n",
    "- **YARN (Hadoop)**: Integra o Spark ao ecossistema Hadoop, permitindo que ele seja executado em clusters Hadoop sob o gerenciamento do YARN (Yet Another Resource Negotiator). Este modo é amplamente utilizado em ambientes de produção, onde o HDFS é o sistema de armazenamento principal, oferecendo fácil escalabilidade e integração com outras ferramentas do Hadoop.\n",
    "\n",
    "- **Mesos**: Apache Mesos é um gerenciador de recursos distribuído que permite executar o Spark em um ambiente multi-tenant com alto controle sobre os recursos. Este modo é apropriado para grandes infraestruturas e é popular em empresas que utilizam vários frameworks de Big Data além do Spark.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Anatomia de Uma Aplicação Spark\n",
    "\n",
    "<br>\n",
    "\n",
    "### Por Que Usar o Spark e suas APIs?\n",
    "\n",
    "Se posso construir meu processo de análise com **Python** ou **R**, por que usaria o **Spark**?\n",
    "\n",
    "- **1.** Porque o Spark é necessário para processar grandes volumes de dados, algo que ferramentas tradicionais podem não suportar eficientemente.\n",
    "- **2.** Porque o Spark oferece APIs especializadas, como **SQL** para consultas estruturadas e **Streaming** para processamento em tempo real, facilitando a construção de soluções de Big Data.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Ecossistema do Apache Spark\n",
    "\n",
    "Assim como o `Hadoop`, o `Spark` possui um ecossistema de módulos que ampliam suas capacidades, cada um voltado para um tipo específico de tarefa:\n",
    "\n",
    "- **Apache Spark Core (engine)**: É o motor de processamento principal, o núcleo do Spark, responsável por tarefas de agendamento, gerenciamento de memória e recuperação de falhas.\n",
    "\n",
    "- **Spark SQL**: API para consulta e processamento de dados estruturados, que permite o uso de **SQL** dentro do Spark. Facilita a integração com dados armazenados em HDFS, Hive, entre outros.\n",
    "\n",
    "- **Spark Streaming**: API para o processamento de dados em tempo real. Ideal para fluxos contínuos de dados, como logs de servidores ou dados de sensores.\n",
    "\n",
    "- **MLlib (Machine Learning Library)**: Biblioteca de machine learning do Spark, que oferece algoritmos e utilitários para aprendizado supervisionado e não supervisionado, como regressão, classificação, clustering, entre outros.\n",
    "\n",
    "- **GraphX (Graph Computation)**: API para a análise de grafos, útil para processar e analisar dados que têm uma estrutura de grafo, como redes sociais e cadeias de influência.\n",
    "\n",
    "<br>\n",
    "\n",
    "> Esses módulos integrados no **Apache Spark** tornam-no uma plataforma versátil para diversas aplicações de Big Data, desde análise exploratória e processamento em tempo real até machine learning e análise de grafos.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Arquitetura da Aplicação\n",
    "\n",
    "Uma aplicação Spark utiliza uma arquitetura de três camadas principais, que facilita a execução distribuída de grandes volumes de dados:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. **API (Scala, Java, Python)**: Esta camada permite que o usuário desenvolva sua aplicação em várias linguagens de programação suportadas pelo Spark (Scala, Java, Python e R), proporcionando flexibilidade e integração com diferentes ecossistemas de dados.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Gestão de Recursos (Distributed Computing)**: Esta camada é responsável por coordenar o uso de recursos do cluster. O Spark pode ser configurado em modos diferentes, dependendo do sistema de gerenciamento de recursos disponível:\n",
    "   - **Standalone**: O Spark utiliza seu próprio cluster manager.\n",
    "   - **YARN (Yet Another Resource Negotiator)**: Integra-se com o Hadoop para o gerenciamento de recursos.\n",
    "   - **Mesos**: Um sistema de gerenciamento de clusters que permite ao Spark compartilhar recursos com outras aplicações.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Dados (Storage)**: Nesta camada, o Spark acessa os dados armazenados em diferentes formatos e sistemas de armazenamento:\n",
    "   - **HDFS (Hadoop Distributed File System)**: O sistema de arquivos distribuído do Hadoop.\n",
    "   - **Outros formatos e bancos de dados**: Suporte para fontes de dados como Cassandra, HBase, S3, JDBC e sistemas de armazenamento locais.\n",
    "\n",
    "<br>\n",
    "\n",
    "> A arquitetura do **Spark** permite o processamento paralelo em clusters distribuídos, maximizando a eficiência e o desempenho, especialmente em grandes volumes de dados. Ao combinar estas três camadas, o Spark oferece uma plataforma robusta para diversas aplicações de Big Data.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Processamento de Uma Aplicação Spark\n",
    "\n",
    "<br>\n",
    "\n",
    "O processamento de uma aplicação Spark ocorre em várias etapas que envolvem o **Driver Program**, o **Cluster Manager** e os **Worker Nodes**:\n",
    "\n",
    "- **Driver Program**: O programa principal que o usuário executa. Ele contém o `SparkContext` que é responsável por estabelecer a comunicação com o cluster e enviar tarefas para processamento.\n",
    "\n",
    "- **Cluster Manager**: Responsável pela alocação e gerenciamento dos recursos no cluster. Ele distribui os recursos necessários para que o Spark execute as tarefas nos Worker Nodes.\n",
    "\n",
    "- **Worker Nodes**: Cada nó de trabalho possui `Executors` (processos que executam as tarefas) e `Cache` (para armazenar dados em memória). O Executor executa as tarefas atribuídas pelo Cluster Manager e armazena em cache os dados frequentemente acessados, para otimizar o processamento.\n",
    "\n",
    "> O cliente submete a aplicação Spark, e o Cluster Manager planeja, agenda, executa e monitora o processamento distribuído nos Worker Nodes, otimizando o uso de recursos para alcançar alta performance em grandes volumes de dados.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Outras Características do Spark\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Suporte a Funções Além de Map e Reduce**: O Spark vai além das funções tradicionais de Map e Reduce, oferecendo uma gama de operações adicionais para processamento de dados.\n",
    "\n",
    "- **Otimização de Operadores de Grafos**: O Spark otimiza o uso de operadores para análise de grafos, tornando-o eficiente para operações complexas de grafos com o módulo `GraphX`\n",
    "\n",
    "- **Avaliação Sob Demanda**: O Spark permite avaliação sob demanda para consultas de Big Data, o que contribui para a otimização e eficiência do fluxo de processamento.\n",
    "\n",
    "- **APIs Concisas e Consistentes**: Fornece APIs consistentes e de fácil uso em `Scala` `Java` e `Python` permitindo a integração com diferentes linguagens de programação.\n",
    "\n",
    "- **Shell Interativo**: O Spark oferece um shell interativo para `Scala`, `Python` e `R`, facilitando testes e desenvolvimento em tempo real. Atualmente, não há suporte para um shell interativo em `Java`.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## RDDs e Dataframes\n",
    "\n",
    "<br>\n",
    "\n",
    "O **Apache Spark** oferece duas estruturas de dados principais para manipulação e processamento de dados: **RDDs** e **DataFrames**. Cada uma dessas estruturas tem características e casos de uso específicos, que se adaptam a diferentes tipos de dados e necessidades de processamento.\n",
    "\n",
    "<br>\n",
    "\n",
    "### O Que São RDDs?\n",
    "\n",
    "**RDD (Resilient Distributed Dataset)** é a estrutura de dados fundamental no Spark, projetada para processamento distribuído de grandes volumes de dados. Um **RDD** é uma coleção de objetos distribuída em vários nós do cluster que pode ser processada em paralelo.\n",
    "\n",
    "- **Tipo de Dados**: Ideal para dados **não estruturados** ou **semi-estruturados**.\n",
    "- **Quando Usar**: Utilize RDDs quando precisar de mais controle sobre o processamento e quando estiver lidando com dados complexos ou heterogêneos que não seguem uma estrutura tabular.\n",
    "- **Vantagens**: Alta flexibilidade, permite manipulação de dados em qualquer formato e processamento paralelo.\n",
    "- **Desvantagens**: Menor otimização em relação a DataFrames e menos eficiente em operações SQL e de consulta estruturada.\n",
    "- **Criação de RDDs**: Existem duas formas de criar um RDD:\n",
    "  - **Paralelizando uma coleção existente**: Com a função `sc.parallelize()`.\n",
    "  - **Referenciando um dataset externo**: A partir de fontes como **HDFS**, **RDBMS**, **NoSQL**, **S3**, entre outros.\n",
    "\n",
    "#### Operações em RDDs\n",
    "\n",
    "O RDD suporta dois tipos principais de operações:\n",
    "\n",
    "- **Transformações**: Operações que produzem um novo RDD a partir de um RDD existente, como `map()`, `filter()`, `flatMap()`, `reduceByKey()` e `aggregateByKey()`.\n",
    "- **Ações**: Operações que computam um resultado a partir de um RDD, como `reduce()`, `collect()`, `first()`, `take()` e `countByKey()`.\n",
    "\n",
    "<br>\n",
    "\n",
    "### O Que São DataFrames?\n",
    "\n",
    "**DataFrames** são uma abstração de alto nível construída sobre os RDDs, projetada para manipular dados **estruturados** e **semi-estruturados**. Um DataFrame é similar a uma tabela de um banco de dados relacional, com linhas e colunas, e suporta consultas SQL, tornando-o mais eficiente para manipulação e análise de dados estruturados.\n",
    "\n",
    "- **Tipo de Dados**: Ideal para dados **estruturados** e **semi-estruturados**.\n",
    "- **Quando Usar**: Utilize DataFrames para operações de consulta estruturada, análises estatísticas e manipulação de dados tabulares.\n",
    "- **Vantagens**: Maior eficiência em operações de consulta, suporte a otimizações internas (Catalyst Optimizer) e integração com SQL.\n",
    "- **Desvantagens**: Menor flexibilidade em relação a RDDs, sendo mais adequado para dados que seguem um formato estruturado.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Diferenças Entre RDDs e DataFrames\n",
    "\n",
    "| Característica            | RDDs                                  | DataFrames                               |\n",
    "|---------------------------|----------------------------------------|------------------------------------------|\n",
    "| **Tipo de Dados**         | Não estruturados e semi-estruturados  | Estruturados e semi-estruturados         |\n",
    "| **Otimização**            | Menos otimizado                       | Otimizado com Catalyst Optimizer         |\n",
    "| **Operações**             | API de baixo nível                    | API de alto nível com suporte a SQL      |\n",
    "| **Controle**              | Mais controle sobre o processamento   | Menos controle, mas mais intuitivo       |\n",
    "| **Desempenho**            | Menos eficiente para consultas SQL    | Mais eficiente para consultas estruturadas|\n",
    "| **Casos de Uso**          | Processamento de dados não estruturados | Análise de dados tabulares e estruturados|\n",
    "\n",
    "<br>\n",
    "\n",
    "### Quando Usar RDDs ou DataFrames?\n",
    "\n",
    "A escolha entre RDDs e DataFrames depende do tipo de dados e da natureza do processamento:\n",
    "\n",
    "- **Use RDDs** quando:\n",
    "  - Precisa de maior controle sobre o processamento.\n",
    "  - Trabalha com dados não estruturados ou heterogêneos.\n",
    "  - Está desenvolvendo operações personalizadas de transformação de dados.\n",
    "\n",
    "- **Use DataFrames** quando:\n",
    "  - Trabalha com dados estruturados, como tabelas ou arquivos CSV.\n",
    "  - Precisa realizar consultas SQL e operações de análise de dados tabulares.\n",
    "  - Deseja aproveitar as otimizações do Catalyst Optimizer para melhorar o desempenho.\n",
    "\n",
    "<br>\n",
    "\n",
    "> Em resumo, **RDDs** são mais flexíveis e oferecem maior controle, sendo úteis para dados não estruturados, enquanto **DataFrames** são otimizados para dados estruturados e oferecem uma interface mais amigável para consultas e operações tabulares no Spark.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c587c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

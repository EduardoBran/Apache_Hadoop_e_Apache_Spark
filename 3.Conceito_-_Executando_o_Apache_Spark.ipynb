{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1077770e",
   "metadata": {},
   "source": [
    "# <center><span style=\"font-size: 42px;color: darkgreen;\">Executando o Apache Spark</span></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "# Execução do Apache Spark em Modo Standalone\n",
    "\n",
    "<br>\n",
    "\n",
    "### Acessando o Spark via Shell\n",
    "\n",
    "Existem duas formas de acessar o Spark via shell, dependendo da linguagem que deseja utilizar:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. **Acessando o Shell em Scala**\n",
    "\n",
    "   - No terminal, digite o seguinte comando para abrir o shell com a linguagem **Scala**:\n",
    "\n",
    "     ```bash\n",
    "     \n",
    "     spark-shell\n",
    "     \n",
    "     \n",
    "     ```\n",
    "    - Para sair do shell Scala, digite:\n",
    "   \n",
    "     ```scala\n",
    "\n",
    "     :q\n",
    "\n",
    "     ```\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Acessando o Shell em Python**\n",
    "\n",
    "   - Para abrir o shell do Spark com a linguagem **Python**, primeiro, **ative o ambiente Conda onde o Spark está configurado** (no seu caso, `spark_env`):\n",
    "\n",
    "       ```bash\n",
    "       conda activate spark_env\n",
    "       ```\n",
    "\n",
    "     - Depois, digite o seguinte comando para acessar o shell do Spark em Python:\n",
    "\n",
    "       ```bash\n",
    "       pyspark\n",
    "       # Uma vez dentro do shell Python do Spark, você pode executar comandos Spark em Python.\n",
    "       ```  \n",
    "       \n",
    "<br>\n",
    "\n",
    "   - Para sair do shell Python, digite:\n",
    "\n",
    "     ```python\n",
    "     exit()\n",
    "     ```\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Observação**: Lembre-se de desativar o ambiente `spark_env` quando não precisar mais do Spark em Python para retornar ao ambiente padrão do sistema. Para isso, use:\n",
    "\n",
    "   ```bash\n",
    "   conda deactivate\n",
    "   ```\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "## Executando Uma Aplicação de Exemplo\n",
    "\n",
    "<br>\n",
    "\n",
    "Após desenvolver a aplicação para resolver o problema de negócio (análise ou modelo preditivo), não utilizamos o `spark-shell` ou `pyspark`. Em vez disso, usamos o **`spark-submit`**, um binário específico do Spark que envia o `Job` para execução no cluster Spark.\n",
    "\n",
    "Para este exemplo **não é necessário ativar ambiente conda** pois usaremos Scala e não o Python.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Verificando Aplicações de Exemplo do Spark\n",
    "\n",
    "<br>\n",
    "\n",
    "- **1.** Ir ao diretório home do Spark digitando no terminal:\n",
    "\n",
    "   ```code\n",
    "   cd $SPARK_HOME\n",
    "   ```\n",
    "   \n",
    "<br>\n",
    "\n",
    "- **2.** Dentro do diretório do Spark, ir até o diretório examples e seguir:\n",
    "\n",
    "   ```code\n",
    "   cd examples\n",
    "   cd src\n",
    "   cd main\n",
    "   cd python\n",
    "   ls -la \n",
    "   # aqui teremos algumas aplicações prontas para exemplo\n",
    "   ```   \n",
    "   \n",
    "<br>\n",
    "\n",
    "- **3.** Retornar ao diretório examples e seguir :\n",
    "\n",
    "   ```code\n",
    "   cd jars/\n",
    "   ls -la    \n",
    "   # dentro do arquivo spark-examples_2.11-2.4.3.jar temos uma aplicação exemplo de como calcular o valor de PI\n",
    "   ```\n",
    "   \n",
    "<br>\n",
    "   \n",
    "- **4.** Retonar a pasta home:\n",
    "\n",
    "   ```code\n",
    "   cd ~\n",
    "   ```\n",
    "   \n",
    "<br>\n",
    "\n",
    "- **5.** Executando uma aplicação do Apache Spark (iremos executar uma classe que está dentro do arquivo `spark-examples_2.11-2.4.3.jar`):\n",
    "\n",
    "   ```code\n",
    "   spark-submit --class org.apache.spark.examples.SparkPi --master local $SPARK_HOME/examples/jars/spark-examples_2.11-2.4.3.jar\n",
    "   ```\n",
    "\n",
    "<br>\n",
    "\n",
    "- **5.1** Executando **outra** aplicação do Apache Spark (iremos executar uma classe que está dentro do arquivo `spark-examples_2.11-2.4.3.jar`):\n",
    "\n",
    "   ```code\n",
    "   spark-submit --class org.apache.spark.examples.LocalKMeans --master local $SPARK_HOME/examples/jars/spark-examples_2.11-2.4.3.jar\n",
    "   ```\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3c294",
   "metadata": {},
   "source": [
    "# <center>Construindo e Executando Aplicação de MapReduce</center>\n",
    "\n",
    "Neste **exemplo** iremos definir o problema de negócio, gerar a massa de dados, construir a aplicação para o `Apache Spark` e realizar a execução.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Problema de Negócio\n",
    "\n",
    "Desenvolver uma aplicação para o `Apache Spark` que acessa um arquivo de texto, analisa o conteúdo e contabiliza a frequência de ocorrência de cada palavra no documento.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### 1. Ativar Ambiente Conda\n",
    "\n",
    "Primeiro, **ative o ambiente Conda onde o Spark está configurado** (no seu caso, `spark_env`):\n",
    "\n",
    "```bash\n",
    "conda activate spark_env    |    conda deactivate\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2. Gerar Massa de Dados\n",
    "\n",
    "Ir ao terminal e digitar:\n",
    "\n",
    "```bash\n",
    "gedit input.txt\n",
    "```\n",
    "\n",
    "Cole um texto qualquer e salve o arquivo.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3. Desenvolver Aplicação\n",
    "\n",
    "Escrever o código utilizando a linguagem `Python` e criar o arquivo `app.py`:\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Criar SparkContext\n",
    "\tconf = SparkConf().setAppName(\"Conta Palavras\").set(\"master\", \"local\")\n",
    "\tsc = SparkContext(conf = conf)\n",
    "\n",
    "\n",
    "\t# Carregar o Arquivo de Texto (está no diretório local, caso estivesse no HDFS deve ser informado)\n",
    "\tpalavras = sc.textFile(\"/home/hadoop/input.txt\")\n",
    "\n",
    "\t# Quebrar cada linha do texto em palavras individuais, utilizando o espaço como delimitador\n",
    "\tpalavras = palavras.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "\n",
    "\t# Mapear cada palavra para uma contagem de 1\n",
    "\tcontagem = palavras.map(lambda palavra: (palavra, 1))\n",
    "\n",
    "\t# Reduzir por chave (palavra) para somar as contagens\n",
    "\tresultado = contagem.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "\t# Salvar o resultado em um novo arquivo \n",
    "\tresultado.saveAsTextFile(\"/home/hadoop/saida\")\n",
    "    \n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3. Executando a Aplicação\n",
    "\n",
    "Navegar até o diretório do arquivo `app.py` e digitar no terminal:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "spark-submit app.py\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4. Verificando Resultado\n",
    "\n",
    "Navegar até o diretório `saida` criado na etapa 2 e visualize:\n",
    "\n",
    "```bash\n",
    "cat /home/hadoop/saida/part-00000\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "# Execução do Apache Spark Remotamente\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Como Executar Jobs Remotamente no Cluster Spark\n",
    "\n",
    "<br>\n",
    "\n",
    "### Contexto\n",
    "\n",
    "No ambiente corporativo, o acesso aos **Clusters Spark** é frequentemente realizado de maneira **remota**, seja em máquinas de clientes ou em projetos internos. Agora iremos simular esse acesso de forma **remota**, operando no modelo **cliente-servidor**.\n",
    "\n",
    "Para este exemplo, temos uma **Máquina Virtual** configurada via `Oracle VirtualBox`, pronta para executar produtos do ecossistema `Hadoop`. Na **etapa anterior**, criamos e executamos a aplicação `app.py` diretamente no servidor. Agora, **acessaremos o servidor remotamente a partir da máquina local (física) para executar a mesma aplicação**.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Conectar-se remotamente ao servidor e executar a aplicação `app.py` desenvolvida na etapa anterior.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### 1. Configurando Apache Spark na Máquina Virtual\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1.1 Copiando os Arquivos Template\n",
    "\n",
    "- Ir ao diretório `/opt/spark/conf` e copiar os arquivos `spark-env.sh.template` e `slaves.template` para `spark-env.sh` e `slaves`, respectivamente:\n",
    "\n",
    "```code\n",
    "cp spark-env.sh.template spark-env.sh\n",
    "cp slaves.template slaves\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### 1.2 Obtendo o IP da Máquina Local\n",
    "\n",
    "- Execute o comando `ifconfig` ou `ip addr` na sua **máquina virtual**. Isso exibirá as interfaces de rede ativas e os respectivos endereços IP. O IP usado, no seu caso `192.168...`, é o endereço da rede local que permite que a máquina virtual acesse a máquina física.\n",
    "\n",
    "\n",
    "```code\n",
    "ifconfig\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### 1.3 Configurando os Arquivos `spark-env.sh` e `slaves`\n",
    "\n",
    "- No arquivo `spark-env.sh`, adicione as seguintes linhas:\n",
    "\n",
    "```code\n",
    "export SPARK_LOCAL_IP=192.168...\n",
    "export SPARK_MASTER_HOST=192.168...\n",
    "```\n",
    "\n",
    "- No arquivo `slaves`, adicione as seguintes linhas:\n",
    "\n",
    "```code\n",
    "192.168...\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### 1.4 Reinicializando e Testando o Spark\n",
    "\n",
    "- No terminal, execute o comando no diretório `/opt/spark/sbin` do Spark para iniciar o **Master** e os **Workers**:\n",
    "\n",
    "```code\n",
    "./start-all.sh\n",
    "jps\n",
    "```\n",
    "\n",
    "- Direto do diretório home:\n",
    "\n",
    "```code\n",
    "/opt/spark/sbin/start-all.sh\n",
    "jps\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- Agora **teste na máquina local** abrindo o navegador e digite:\n",
    "\n",
    "```code\n",
    "192.168...:8080\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### 2. Configurando Apache Spark na Máquina Local\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Importante**: É necessário realizar o **download** e fazer a **instalação** do `Apache Spark` da **mesma versão** da **máquina virtual**.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.1 Instalando o Spark na Máquina Local\n",
    "\n",
    "- Navegar até o diretório onde está o arquivo do `Spark` e digitar no terminal:\n",
    "\n",
    "```code\n",
    "tar -xzvf spark-2.4.4-bin-hadoop2.7.tgz\n",
    "sudo mv spark-2.4.4-bin-hadoop2.7 /opt/\n",
    "sudo mv /opt/spark-2.4.4-bin-hadoop2.7 /opt/spark\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- Retornar ao diretório `home` e editar as variáveis de ambiente:\n",
    "\n",
    "```code\n",
    "gedit .bashrc\n",
    "\n",
    "# Colar as linhas:\n",
    "# Spark\n",
    "export SPARK_HOME=/opt/spark\n",
    "export PATH=$PATH:$SPARK_HOME/bin\n",
    "#export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "#export PYSPARK_DRIVER_PYTHON_OPTS=notebook\n",
    "#export PYSPARK_PYTHON=python3\n",
    "\n",
    "source .bashrc\n",
    "```\n",
    "<br>\n",
    "\n",
    "- Testar conectando na máquina local:\n",
    "\n",
    "```code\n",
    "spark-shell\n",
    ":q\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### 3. Acesso ao Cluster Spark (Hospedado na Máquina Virtual) a Partir da Máquina Local\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.1 Estabelecendo Conexão Remota com o Cluster Spark na Máquina Virtual\n",
    "\n",
    "- No terminal da **máquina virtual** inicie os serviços:\n",
    "\n",
    "```code\n",
    "/opt/spark/sbin/start-all.sh\n",
    "jps\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- No terminal da **máquina local** digitar para se conectar:\n",
    "\n",
    "```code\n",
    "spark-shell --master spark://192.168.0.19:7077\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- Acessar via navegador:\n",
    "\n",
    "```code\n",
    "192.168...:8080\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4. Executando Jobs Remotamente\n",
    "\n",
    "<br>\n",
    "\n",
    "- Escrever o código utilizando a linguagem `Python` e criar o arquivo `app_local.py`:\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Criar SparkContext\n",
    "    conf = SparkConf().setAppName(\"Conta Palavras\").setMaster(\"spark://192.168.0.19:7077\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "    # Carregar o Arquivo de Texto (está no diretório local, caso estivesse no HDFS deve ser informado)\n",
    "    palavras = sc.textFile(\"/home/eduardo/Compartilhada/input.txt\")\n",
    "\n",
    "    # Quebrar cada linha do texto em palavras individuais, utilizando o espaço como delimitador\n",
    "    palavras = palavras.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "    # Mapear cada palavra para uma contagem de 1\n",
    "    contagem = palavras.map(lambda palavra: (palavra, 1))\n",
    "\n",
    "    # Reduzir por chave (palavra) para somar as contagens\n",
    "    resultado = contagem.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    # Salvar o resultado em um novo arquivo \n",
    "    resultado.saveAsTextFile(\"/home/eduardo/Compartilhada/saida10\")\n",
    "```\n",
    "<br>\n",
    "\n",
    "- Ir até o diretório do arquivo `app_local.py` e digitar no terminal:\n",
    "\n",
    "```code\n",
    "spark-submit app_local.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4752a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

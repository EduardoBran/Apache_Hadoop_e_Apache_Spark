{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9ec29e",
   "metadata": {},
   "source": [
    "# <center><span style=\"font-size: 42px;color: darkgreen;\">Componentes do Apache Spark</span></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Apache Spark SQL\n",
    "\n",
    "**Apache Spark SQL** é um módulo do Apache Spark para processamento de dados estruturados, permitindo consultas SQL sobre conjuntos de dados. Ele suporta operações ETL em diversos formatos, como JSON, CSV e bancos de dados relacionais ou NoSQL. Além disso, o Spark SQL facilita o uso de SQL em aplicações de análise de dados escritas em Python, Scala e outras linguagens.\n",
    "\n",
    "As funcionalidades principais incluem:\n",
    "- **DataFrames**: Uma abstração que permite manipulação de dados tabulares com operações SQL e alto nível de otimização.\n",
    "- **SparkSession**: O ponto de entrada para todas as funcionalidades do Spark SQL, facilitando a criação de DataFrames e a execução de queries SQL.\n",
    "- **SQLContext e HiveContext**: Permitem o uso de consultas SQL no Spark, com HiveContext oferecendo maior funcionalidade para manipulação de dados do Hive.\n",
    "- **JDBC**: Integração com bancos de dados relacionais para consulta e manipulação de dados estruturados.\n",
    "- **Tabelas Temporárias**: Permite consultas SQL em tabelas temporárias, retornando resultados como DataFrames.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Apache Spark MLlib\n",
    "\n",
    "**Spark MLlib** é a biblioteca de aprendizado de máquina do Spark, projetada para executar algoritmos de machine learning de forma distribuída e escalável.\n",
    "\n",
    "Principais funcionalidades:\n",
    "- **Classificação**: Algoritmos para detectar padrões e classificar dados (como identificar fraude e detectar spam).\n",
    "- **Regressão**: Modelos que identificam relações entre variáveis e fazem previsões.\n",
    "- **Clusterização**: Algoritmos para agrupar dados em categorias com base em características comuns.\n",
    "- **Filtragem Colaborativa**: Recomendação de itens com base em preferências de usuários.\n",
    "\n",
    "**Spark MLlib** suporta a criação de modelos preditivos em RDDs e DataFrames. Ele também fornece APIs para extração de características, permitindo que dados textuais sejam convertidos para representações numéricas, e inclui funções para avaliar modelos.\n",
    "\n",
    "### Apache Spark MLlib x Apache Mahout\n",
    "\n",
    "O **Apache Mahout** é o framework de machine learning do Hadoop, enquanto o **Spark MLlib** é o módulo de machine learning do Spark. O MLlib executa sobre o Spark, tornando-o mais rápido que o Mahout, que roda no Hadoop MapReduce. Ambos são open-source e suportam paralelização, mas o MLlib é mais indicado para quem está começando em machine learning para Big Data.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Apache Spark Streaming\n",
    "\n",
    "O **Spark Streaming** permite o processamento de dados em tempo real, com suporte para múltiplas fontes de dados (TCP/IP, Kafka, Flume, arquivos, redes sociais, entre outras). Utilizando micro-batches, ele simula o processamento contínuo, tornando-o útil para monitoramento e detecção de eventos em tempo quase real. \n",
    "\n",
    "> Embora não seja \"real real-time\", o Spark Streaming se aproxima disso ao processar dados em micro-lotes, proporcionando agilidade em muitas aplicações.\n",
    "\n",
    "### Processamento em Batch x Processamento de Stream\n",
    "\n",
    "- **Batch**: Análise de dados acumulados, usada para exploração de dados, construção de data warehouses e treinamento de modelos.\n",
    "- **Stream**: Processamento em tempo real, útil para monitoramento de serviços, processamento de eventos de sensores, e análises em tempo real.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Apache Spark GraphX\n",
    "\n",
    "**GraphX** é o módulo de processamento de grafos do Spark, que permite a análise paralela e distribuída de grafos. Um grafo é uma estrutura matemática com vértices e arestas, usada para representar redes interconectadas, como redes sociais ou fluxos logísticos.\n",
    "\n",
    "GraphX expande o conceito de RDDs com os **Resilient Distributed Property Graphs**, uma estrutura para processamento de elementos de grafos (como vértices e arestas) de forma distribuída. Ele é utilizado em algoritmos de grafos para tarefas como PageRank e filtros colaborativos.\n",
    "\n",
    "> Observação: o GraphX está disponível apenas em Scala e é um recurso avançado para análise de Big Data com grafos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbfd4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
